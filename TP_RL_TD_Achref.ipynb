{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ztJNm_1JnIA"
      },
      "source": [
        "#TP Méthodes des différences temporelles -- SARSA et Q-Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfPlRr2lJ1ul"
      },
      "source": [
        "## Frozen Lake\n",
        "\n",
        "L'environnement Frozen Lake est un monde grille incertain dans lequel on part depuis un état initial (la case la plus en haut à gauche) pour aller à un état final (la case la plus basse à droite). L'environnement est incertain car vous marchez sur un lac gelé et l'épaisseur de glace varie. Vous pouvez donc tomber dans l'eau dans certaines cases. De plus, la glace est plus glissante à certains endroits, et donc faire un pas peut vous mener plus loin que prévu... et si le vent s'en mèle...\n",
        "\n",
        "Au lieu d'essayer d'estimer le modèle de transition, on va utiliser SARSA et Q-learning pour résoudre ce problème.\n",
        "\n",
        "Utilisez l'environnement Frozen Lake pour implémenter SARSA et Q-learning. Utilisez d'abord l'environnement avec une grille 4x4 pour tester vos algorithmes, puis vous devriez pouvoir les utiliser pour la grille 16x16.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxXBD-woAxNm"
      },
      "source": [
        "## FrozenLake - familiarisation avec l'environnement\n",
        "\n",
        "Evaluez une politique aléatoire. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYxTls9tJmVX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2ef851f-b375-4448-8604-37c7cddd16b3"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import gym\n",
        "#env = gym.make(\"FrozenLake8x8-v0\",  is_slippery=True)\n",
        "env = gym.make(\"FrozenLake-v1\")\n",
        "numStates = env.observation_space.n\n",
        "numActions = env.action_space.n\n",
        "print(\"Environnement avec \", numStates, \" états et \", numActions, \" actions\")\n",
        "#\n",
        "# env.reset() fait commencer un nouvel épisode\n",
        "# la méthode retourne l'état initial\n",
        "#\n",
        "state = env.reset()\n",
        "nbIt=0\n",
        "rew=[]\n",
        "done=False\n",
        "while not done:\n",
        "  #\n",
        "  # env.step(action) exécute une action dans l'état courant\n",
        "  # la méthode retourne:\n",
        "  #    • l'état suivant\n",
        "  #    • la récompense immédiate\n",
        "  #    • un booléen qui indique si l'épisode est terminé\n",
        "  #    • le quatrième argument ne nous est pas utile (sert pour le debuggage dans certains cas)\n",
        "  #\n",
        "  nextState, reward, done, info = env.step(np.random.randint(4))\n",
        "  print(\"etat numero:\",nextState)\n",
        "  #\n",
        "  # env.render donne une réprésentation de l'état.\n",
        "  #\n",
        "  #env.render()\n",
        "  nbIt+=1\n",
        "  rew = rew+[reward]\n",
        "print(\"Episode terminé après {} itérations\".format(nbIt))\n",
        "print(\"Récompenses obtenues:\",rew)\n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Environnement avec  16  états et  4  actions\n",
            "etat numero: 1\n",
            "etat numero: 2\n",
            "etat numero: 6\n",
            "etat numero: 7\n",
            "Episode terminé après 4 itérations\n",
            "Récompenses obtenues: [0.0, 0.0, 0.0, 0.0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:318: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gym[toy_text]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPOZVkdKnf4Y",
        "outputId": "4ef77d2a-8920-45ae-a86b-a785bf446136"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[toy_text] in /usr/local/lib/python3.7/dist-packages (0.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[toy_text]) (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[toy_text]) (1.21.6)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym[toy_text]) (0.0.8)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym[toy_text]) (4.13.0)\n",
            "Collecting pygame==2.1.0\n",
            "  Downloading pygame-2.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 18.3 MB 111 kB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[toy_text]) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[toy_text]) (4.1.1)\n",
            "Installing collected packages: pygame\n",
            "Successfully installed pygame-2.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCR8A5xKB1Zv"
      },
      "source": [
        "## $\\epsilon$-greedy\n",
        "\n",
        "Implémentez une fonction qui retourne une action avec la stratégie $\\epsilon$-greedy:\n",
        "* exploite avec un probabilité $1-\\epsilon$: ici on choisit l'action avec la meilleure valeur de $q[s]$\n",
        "* explore avec une probabilité $\\epsilon$: on choisit une action de manière uniforme sur toute les actions.\n",
        "\n",
        "Vous pouvez choisir différente signature pour la fonction:\n",
        "soit en lui passant:\n",
        " * le paramètre $\\epsilon$\n",
        " * la table `Q`\n",
        " * l'état `s` dans lequel l'action sera exécutée\n",
        " * donc l'appel aura la forme `action = epsGreedy(eps, Q, s)`\n",
        "\n",
        " Autre solution, vous pouvez donner seulement la valeur de $\\epsilon$ et vecteur Q(s) (qui a pour dimension le nombre d'actions). L'appel aura donc la forme `action = epsGreedy(eps, q)`\n",
        "\n",
        "*Attention* On peut imaginer le cas particulier où on trouve plusieurs occurrence de la valeur max dans le vecteur `Q(s)`. Dans ce cas, il ne faudrait pas *toujours* choisir la même action, mais plutôt choisir une des actions ex-aequo au hasard. \n",
        "Ce cas n'est peut-être pas si exotique que cela, en particulier en début d'apprentissage, quand toutes les valeurs sont nulles. Pour explorer, il est alors souhaitable de répéter le même choix!\n",
        "\n",
        "\n",
        "Pour ceux peu familier avec python, regardez le petit exemple de code ci-dessous pour illustrer quelques fonctions de la bibliothèque `numpy`\n",
        "- La fonction `np.random.rand()` tire une valeur de manière uniforme entre 0 et 1. \n",
        "- La fonction np.random.choice permet de choisir de manière uniforme une valeur parmi un ensemble.\n",
        "- La fonction `np.argwhere(l)` permet de donner les indices où l'entrée du vecteur l est non nul. On peut donc coupler un appel de `np.argwhere` avec un test.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Otp_pfpaske",
        "outputId": "efd57d33-02db-4e4e-a1ac-bd21c152c123"
      },
      "source": [
        "val = np.random.rand()\n",
        "print(val)\n",
        "val=np.zeros(10)\n",
        "# on tire au sort 10 fois une valeur dans l'ensemble {1, 3, 5}\n",
        "for i in range(10):\n",
        "  val[i]=np.random.choice([1,3,5])\n",
        "_, count = np.unique(val,return_counts=True)\n",
        "print(\"résultat des 10 tirages:\", val)\n",
        "print(\"la proportion de chaque valeur sur ces 10 tirages est \", count/10)\n",
        "indices3 = np.argwhere(val==3)\n",
        "print(\"Les indices de la liste val où la valeur est 3 sont:\", indices3)\n",
        "\n",
        "def eps_greedy(qtable, epsilon, n_actions, state):\n",
        "   if np.random.random() <= epsilon:\n",
        "      return np.random.randint(n_actions)\n",
        "   else:\n",
        "      return np.argmax(qtable[state, :])\n",
        "        \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.23010303640948337\n",
            "résultat des 10 tirages: [5. 3. 3. 5. 5. 5. 3. 1. 3. 5.]\n",
            "la proportion de chaque valeur sur ces 10 tirages est  [0.1 0.4 0.5]\n",
            "Les indices de la liste val où la valeur est 3 sont: [[1]\n",
            " [2]\n",
            " [6]\n",
            " [8]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qtable = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "for i in range(1000):\n",
        "  state=env.reset()\n",
        "  eps_greedy(qtable,0.9,4,state)\n",
        "qtable  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRAUwB-YD2gG",
        "outputId": "17362322-6bbb-4432-c2e2-404859de5139"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HKc6m_afc89"
      },
      "source": [
        "## Tester une politique\n",
        "\n",
        "Lors de l'apprentissage, il est nécessaire d'explorer, ainsi lorsqu'on analyse les performances durant l'apprentissage, il faut avoir à l'esprit qu'une partie des choix est faite au hasard. Après avoir appris, on peut faire un test en étant glouton: à chaque état, on choisit toujours l'action qui donne la plus haute valeur de `Q`.\n",
        "Implémentez une méthode qui prend en paramètre un nombre d'épisodes fixé, une table `Q`, et qui exécute la politique gloutone. La méthode retourne la valeur moyenne de la somme des récompenses sur l'épisode."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOYzTAq3gZxY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a05c9bbf-cb75-4909-a54f-d76d496a0d1b"
      },
      "source": [
        "qtable = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "episodes =100\n",
        "epsilon=1\n",
        "alpha =0.02\n",
        "gamma=0.9\n",
        "success=0\n",
        "\n",
        "for i in range(1000):\n",
        "  state = env.reset()\n",
        "  a=eps_greedy(qtable,epsilon,4,state)\n",
        "  done= False\n",
        "  while not done:\n",
        "    s_, reward, done, _ = env.step(a)\n",
        "    a_=eps_greedy(qtable,epsilon,4,s_)\n",
        "    qtable[state,a] += alpha * (reward + (gamma * qtable[s_, a_]) - qtable[state, a])\n",
        "    s=s_\n",
        "    a=a_\n",
        "    success += reward\n",
        "    \n",
        "    \n",
        "env.close()\n",
        "qtable\n",
        "\n",
        "\n",
        "    \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.00113464, 0.00016079, 0.00598269, 0.00082695],\n",
              "       [0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "success_rate=success/1000\n",
        "print(success_rate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtJJPrEICmmi",
        "outputId": "ce38b69f-d5bb-4c4c-bfff-5da960c1bc36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.015\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lXfcmn4E-5kT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JISzKV0alpV"
      },
      "source": [
        "### SARSA\n",
        "\n",
        "Implémentez un fonction SARSA qui prend en paramètre\n",
        " * un nombre d'épisodes utilisés pour l'apprentissage\n",
        " * $\\gamma$ le taux d'escompte\n",
        " * $\\alpha$ le taux d'apprentissage (que l'on retrouve lors de la mise à jour des valeurs de Q)\n",
        " * $\\epsilon$ le paramètre pour la méthode $\\epsilon$-greedy.\n",
        "\n",
        "Votre fonction doit au moins retourner la table $Q: S \\times A$. Vous trouverez ci-dessous une fonction $plotQ$ qui génère une représentation de la table $Q$: pour chaque case sera dessiné la meilleure action selon $Q$ et la couleur représentera la valeur de cette action.\n",
        " \n",
        "Pour visualiser les progrès faits pendant l'apprentissage, votre fonction SARSA peut également retourner une séquence de valeurs. Par exemple,\n",
        " * la séquence de récompenses (totale ou moyenne) obtenue sur chaque épisode de l'apprentissage\n",
        " * la valeur de la meilleure action pour l'état de départ à chaque fin d'épisode.\n",
        " * au lieu d'utiliser les valeurs obtenues lors de l'apprentissage, vous pouvez aussi effectuer périodiquement une évaluation de la politique courante (sans exploration). Pour ce faire, vous pouvez calculer la performance sur un petit nombre d'épisodes et retourner la moyenne. Cette méthode a l'avantage d'évaluer la politique sans exploration (donc une meilleure évaluation de la politique), mais peut coûter cher en temps de calcul suivant la fréquence d'exécution et le nombre d'épisodes utilisés pour l'évaluation.\n",
        "\n",
        "En générant le graphique, vous devriez visualiser si l'algorithme est arrivé à améliorer les performances. Vous pouvez soit tracer directement la valeur de chaque épisode. Pour avoir une courbe un peu plus lisse, vous pouvez aussi calculer une moyenne sur une fenêtre de $k$ épisodes (la fonction $runningAvg$ effectue ce travail).\n",
        "\n",
        "Notez qu'on considère Frozen lake comme résolu quand\n",
        " * il atteint le but dans 78% des épisodes pour la grille 4x4.\n",
        " * a priori, on peut atteindre 100% pour la grille 8x8\n",
        "\n",
        "Quelques idées pour aider au debeug:\n",
        " * vous pouvez aussi regarder si la plupart des paires état-actions ont été exécutée. \n",
        " * Vous pouvez choisir comme paramètres (le code que j'ai écrit a fonctionné avec ces paramètres, évidemment, vous pouvez essayer avec d'autres par la suite).\n",
        "   * $\\epsilon=0.2$\n",
        "   * $\\alpha=0.02$\n",
        "   * Frozen lake est une tâche épisodique, donc ici, on peut s'intéresser simplement à la somme des récompenses accumulées lors d'un épisode. Donc on peut choisir $\\gamma=1$ (pas d'escompte)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def choose_action(state):\n",
        "\taction=0\n",
        "\tif np.random.uniform(0, 1) < epsilon:\n",
        "\t\taction = env.action_space.sample()\n",
        "\telse:\n",
        "\t\taction = np.argmax(Q[state, :])\n",
        "\treturn action\n",
        "\n",
        "def learn(state, state2, reward, action, action2):\n",
        "\tpredict = Q[state, action]\n",
        "\ttarget = reward + gamma * Q[state2, action2]\n",
        "\tQ[state, action] = Q[state, action] + lr_rate * (target - predict)"
      ],
      "metadata": {
        "id": "wmwTVHvPOX6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "epsilon = 0.2\n",
        "total_episodes = 10000\n",
        "max_steps = 100\n",
        "lr_rate = 0.02\n",
        "gamma = 1\n",
        "\n",
        "Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "    \n",
        "rewards=0\n",
        "\n",
        "for episode in range(total_episodes):\n",
        "\tt = 0\n",
        "\tstate = env.reset()\n",
        "\taction = choose_action(state)\n",
        "    \n",
        "\twhile t < max_steps:\n",
        "\t\n",
        "\n",
        "\t\tstate2, reward, done, info = env.step(action)\n",
        "\n",
        "\t\taction2 = choose_action(state2)\n",
        "\n",
        "\t\tlearn(state, state2, reward, action, action2)\n",
        "\n",
        "\t\tstate = state2\n",
        "\t\taction = action2\n",
        "\n",
        "\t\tt += 1\n",
        "\t\trewards+=1\n",
        "\n",
        "\t\tif done:\n",
        "\t\t\tbreak\n",
        "  \n",
        "\n",
        "    \n",
        "print (\"Score over time: \", rewards/total_episodes)\n",
        "print(Q)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYcdudnRMKge",
        "outputId": "84097da0-aa8f-4e50-8c87-d03ca9062ab4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score over time:  18.32\n",
            "[[0.24726913 0.21915649 0.21817943 0.21327247]\n",
            " [0.13187978 0.13004694 0.10694792 0.18414812]\n",
            " [0.16606145 0.12774713 0.11577697 0.10873035]\n",
            " [0.00590595 0.05326795 0.00388758 0.01159494]\n",
            " [0.26240479 0.18554864 0.16783445 0.15761809]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.14377359 0.08659188 0.14098436 0.0530235 ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.16926198 0.24675745 0.20163162 0.29941134]\n",
            " [0.27476796 0.4120388  0.28504613 0.24762128]\n",
            " [0.40786575 0.37346903 0.23382787 0.15820014]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.26297223 0.4137163  0.54131163 0.37597208]\n",
            " [0.53892376 0.72966687 0.6397551  0.60721624]\n",
            " [0.         0.         0.         0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBPUTjtTXpyZ"
      },
      "source": [
        "def runningAvg(data, windowSize):\n",
        "  res = np.zeros(len(data)-windowSize)\n",
        "  sum=0\n",
        "  for i in range(windowSize):\n",
        "    sum += data[i]\n",
        "  for i in range(len(data)-windowSize):\n",
        "    res[i]= sum/windowSize\n",
        "    sum -= data[i]\n",
        "    sum += data[i+windowSize]\n",
        "  return res\n",
        "\n",
        "\n",
        "# visualisation de la table Q pour FrozenLake 4x4 et 8x8\n",
        "# passez la taille (4 ou 8) en paramètres\n",
        "def plotQ(q_table, map_size):\n",
        "  if (map_size==4):\n",
        "    MAP = [\n",
        "        \"SFFF\",\n",
        "        \"FHFH\",\n",
        "        \"FFFF\",\n",
        "        \"HFFG\"\n",
        "    ]\n",
        "  else:\n",
        "    MAP=[\n",
        "        \"SFFFFFFF\",\n",
        "        \"FFFFFFFF\",\n",
        "        \"FFFHFFFF\",\n",
        "        \"FFFFFHFF\",\n",
        "        \"FFFHFFFF\",\n",
        "        \"FHHFFFHF\",\n",
        "        \"FHFFHFHF\",\n",
        "        \"FFFHFFFG\"\n",
        "    ]\n",
        "  best_value = np.max(q_table, axis = 1).reshape((map_size,map_size))\n",
        "  best_policy = np.argmax(q_table, axis = 1).reshape((map_size,map_size))\n",
        "    \n",
        "  fig, ax = plt.subplots()\n",
        "  im = ax.imshow(best_value)\n",
        "\n",
        "  for i in range(best_value.shape[0]):\n",
        "      for j in range(best_value.shape[1]):\n",
        "          if MAP[i][j] in 'GH':\n",
        "              arrow = MAP[i][j]\n",
        "          elif best_policy[i, j] == 0:\n",
        "              arrow = '<'\n",
        "          elif best_policy[i, j] == 1:\n",
        "              arrow = 'v'\n",
        "          elif best_policy[i, j] == 2:\n",
        "              arrow = '>'\n",
        "          elif best_policy[i, j] == 3:\n",
        "              arrow = '^'\n",
        "          if MAP[i][j] in 'S':\n",
        "              arrow = 'S ' + arrow\n",
        "          text = ax.text(j, i, arrow, ha = \"center\", va = \"center\",\n",
        "                         color = \"black\")\n",
        "            \n",
        "  cbar = ax.figure.colorbar(im, ax = ax)\n",
        "    \n",
        "  fig.tight_layout()\n",
        "  plt.show() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plotQ(Q,4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "408YUsDJNsKA",
        "outputId": "757c8494-8559-45e0-a3fb-285028fa9adb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oal2LS2mG3qW"
      },
      "source": [
        "## Q-learning\n",
        "Implémentez l'algorithme Q-learning (en partant de SARSA, il ne devrait y avoir que quelques lignes de codes à modidier!)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZ8bmLd8pxjD"
      },
      "source": [
        "def choose_action(state):\n",
        "    action=0\n",
        "    if np.random.uniform(0, 1) < epsilon:\n",
        "        action = env.action_space.sample()\n",
        "    else:\n",
        "        action = np.argmax(Q[state, :])\n",
        "    return action\n",
        "\n",
        "def learn(state, state2, reward, action):\n",
        "    predict = Q[state, action]\n",
        "    target = reward + gamma * np.max(Q[state2, :])\n",
        "    Q[state, action] = Q[state, action] + lr_rate * (target - predict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epsilon = 0.2\n",
        "total_episodes = 10000\n",
        "max_steps = 100\n",
        "lr_rate = 0.02\n",
        "gamma = 1\n",
        "\n",
        "Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "rewards = 0\n",
        "for episode in range(total_episodes):\n",
        "    state = env.reset()\n",
        "    t = 0\n",
        "    \n",
        "    while t < max_steps:\n",
        "        \n",
        "\n",
        "        action = choose_action(state)  \n",
        "\n",
        "        state2, reward, done, info = env.step(action)  \n",
        "\n",
        "        learn(state, state2, reward, action)\n",
        "\n",
        "        state = state2\n",
        "\n",
        "        t += 1\n",
        "        rewards +=1\n",
        "       \n",
        "        if done:\n",
        "            break\n",
        "\n",
        "        \n",
        "\n",
        "print(Q)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQ173yOXPlA7",
        "outputId": "1a8297ca-b2fc-4df4-e0aa-ea118bd9ee50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.74621715 0.69038361 0.67748297 0.70760257]\n",
            " [0.37971567 0.34936591 0.29473295 0.59657038]\n",
            " [0.47927884 0.20518891 0.20283125 0.22370903]\n",
            " [0.11126798 0.01561491 0.00251317 0.01502057]\n",
            " [0.74737861 0.48454618 0.42466442 0.50921004]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.39573082 0.16096012 0.20109601 0.12490983]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.48251948 0.51746773 0.50395191 0.75158804]\n",
            " [0.51457461 0.76207467 0.55685248 0.45796215]\n",
            " [0.72012187 0.47257844 0.35558776 0.33908536]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.47309911 0.52057716 0.83114926 0.57171652]\n",
            " [0.72459736 0.92712951 0.81183461 0.78057493]\n",
            " [0.         0.         0.         0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plotQ(Q,4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "JfnbaKTrQQEV",
        "outputId": "35e99492-466f-4dd3-c30a-37ef919e82c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nixgMInoHHtz"
      },
      "source": [
        "## Comparaison\n",
        "\n",
        "Comparer les politiques trouvées à l'aide de SARSA, Q-learning, et vous devriez aussi pouvoir utiliser le code de l'algorithme on policy Monte Carlo du TD précédent.\n",
        "\n",
        "Avant convergence à l'optimal, on observe souvent que SARSA a choisi une politique moins risquée avant de tomber sur l'optimal pour le FrozenLake8x8."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMtevQsXpGUX"
      },
      "source": [
        "## Cart-pole en tabulaire\n",
        "\n",
        "On vous propose pour finir d'utiliser votre code et de tester l'apprentissage sur le problème du cart-pole. A priori, c'est un problème où les états sont des variables continues. On vous propose ici de discrétiser les variables et d'essayer d'utiliser une des méthodes pour voir vos résultats. \n",
        "\n",
        "La récompense que vous obtenez est le nombre de pas de temps où le baton est resté en équilibre. Si vous utilisez colab pour coder, vous ne pourrez malheureusement pas visualiser un épisode avec la méthode render :-(\n",
        "\n",
        "Cet environnement Cart-Pole consiste à déplacer un chariot pour faire tenir en équilibre une poutre. Plus précisément:\n",
        "* Il y a deux actions : gauche et droite (représentées par 0 et 1).\n",
        "* L'observation reçue (c'est à dire l'état) est un tableau numpy comprenant 4 variables: la position du chariot, la vélocité, l'angle à la verticale et la position du haut de la poutre.\n",
        "* L'épisode se termine lorsque l'angle de la poutre à la verticale dépasse 12 degrés.\n",
        "* Les récompenses reçues sont égales à 1 sauf si l'angle dépasse 12 degrés.\n",
        "\n",
        "On vous donne ci-dessous les fonctions pour réaliser la discrétisation et pour encoder l'état en un entier.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ep-nyEsPtxi2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "a7c353ae-ee4e-4fec-977f-5ad939cd042c"
      },
      "source": [
        "env = gym.make(\"CartPole-v1\")\n",
        "print(\"environnement avec \", env.action_space.n, \" actions\")\n",
        "print(\"l'espace des états est lui codé avec une class\", env.observation_space,\n",
        "      \" qui représente un espace continu\")\n",
        "print(\"les bornes inférieures des intervalles sont: \", env.observation_space.low)\n",
        "print(\"les bornes supérieures des intervalles sont: \",env.observation_space.high)\n",
        "env.reset()\n",
        "nbIt=0\n",
        "done=False\n",
        "while not done:\n",
        "  observation, reward, done, info = env.step(np.random.randint(2))\n",
        "  nbIt+=1\n",
        "print(\"Episode terminé après {} itérations\".format(nbIt))\n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "environnement avec  2  actions\n",
            "l'espace des états est lui codé avec une class Box(4,)  qui représente un espace continu\n",
            "les bornes inférieures des intervalles sont:  [-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38]\n",
            "les bornes supérieures des intervalles sont:  [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]\n",
            "Episode terminé après 16 itérations\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDMbbzDrqCrV"
      },
      "source": [
        "def discretise(x,mini,maxi): \n",
        "  # discretise x\n",
        "  # renvoie un entier entre 0 et nval-1\n",
        "  if x<mini: x=mini\n",
        "  if x>maxi: x=maxi\n",
        "  return int(np.floor((x-mini)*nval/(maxi-mini+0.0001)))\n",
        "\n",
        "def encode(observation):\n",
        "  pos = discretise(observation[0],mini=-1,maxi=1)\n",
        "  vel = discretise(observation[1],mini=-1,maxi=1)\n",
        "  angle = discretise(observation[2],mini=-1,maxi=1)\n",
        "  pos2 = discretise(observation[3],mini=-1,maxi=1)\n",
        "  return pos + vel*nval + angle*nval*nval + pos2*nval*nval*nval\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0z0VtgYq-fv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ca11696a-3569-4f08-cac8-25d59210e8c6"
      },
      "source": [
        "nval =5 # nombre de valeurs discrètes qu’une variable peut prendre\n",
        "N= nval ** 4 # Puisqu’il y a 4 variables, la taille de l’espace est nval^4\n",
        "print(\"Le nombre d'états sera ici de \", N)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Le nombre d'états sera ici de  625\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Sq12HjSrgNw"
      },
      "source": [
        "Modifiez votre implémentation de Q-learning et/ou de SARSA pour tester si vous pouvez apprendre à maintenier le baton en équilibre. Une modification sera d'utilisé les fonctions ci-dessus pour encoder/decoder un état. Une autre sera surement d'ajouter le nombre d'états en paramètre car ce nombre est maintenant indépendant de l'environnement! \n",
        "Avec comme paramètre $\\epsilon=0.1$, $\\alpha=0.2$ et $\\gamma=0.9$, j'arrive a atteindre un score autour de 90 pas de temps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ythSnUmaIEIs"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}